
#name <Tmp$Dir>.^.^.test_asv_addition_subtraction_instructions

#area test,code

; VADD

VADD.I8    Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 16*8)
VADD.I16   Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 8*16)
VADD.I32   Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 4*32)
VADD.I64   Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 2*64)

VADD.I8    D1,D2,D3 ; D1=D2+D3 (SIMD, 16*8)
VADD.I16   D1,D2,D3 ; D1=D2+D3 (SIMD, 8*16)
VADD.I32   D1,D2,D3 ; D1=D2+D3 (SIMD, 4*32)
VADD.I64   D1,D2,D3 ; D1=D2+D3 (SIMD, 2*64)

VADD.F32   Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 4*32)
VADD.F32   D1,D2,D3 ; D1=D2+D3 (SIMD, 2*32)

VADD.F64   D1,D2,D3 ; D1=D2+D3 (VFP)
VADDEQ.F64 D1,D2,D3 ; D1=D2+D3 (VFP)

VADD.F32   S1,S2,S3 ; S1=S2+S3 (VFP)
VADDEQ.F32 S1,S2,S3 ; S1=S2+S3 (VFP)

; VADDHN

VADDHN.I16 D1,Q2,Q3 ; D1=Q2+Q3 (SIMD, 8*16)
VADDHN.I32 D1,Q2,Q3 ; D1=Q2+Q3 (SIMD, 4*32)
VADDHN.I64 D1,Q2,Q3 ; D1=Q2+Q3 (SIMD, 2*64)

; VADDL

VADDL.S8   Q1,D2,D3 ; Q1=D2+D3 (SIMD, 8*8)
VADDL.S16  Q1,D2,D3 ; Q1=D2+D3 (SIMD, 4*16)
VADDL.S32  Q1,D2,D3 ; Q1=D2+D3 (SIMD, 2*32)
VADDL.U8   Q1,D2,D3 ; Q1=D2+D3 (SIMD, 8*8)
VADDL.U16  Q1,D2,D3 ; Q1=D2+D3 (SIMD, 4*16)
VADDL.U32  Q1,D2,D3 ; Q1=D2+D3 (SIMD, 2*64)

; VADDW

VADDW.S8   Q1,Q2,D3 ; Q1=Q2+D3 (SIMD, 8*8)
VADDW.S16  Q1,Q2,D3 ; Q1=Q2+D3 (SIMD, 4*16)
VADDW.S32  Q1,Q2,D3 ; Q1=Q2+D3 (SIMD, 2*32)
VADDW.U8   Q1,Q2,D3 ; Q1=Q2+D3 (SIMD, 8*8)
VADDW.U16  Q1,Q2,D3 ; Q1=Q2+D3 (SIMD, 4*16)
VADDW.U32  Q1,Q2,D3 ; Q1=Q2+D3 (SIMD, 2*64)

; VHADD

VHADD.S8   Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 16*8)
VHADD.S16  Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 8*16)
VHADD.S32  Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 4*32)
VHADD.U8   Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 16*8)
VHADD.U16  Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 8*16)
VHADD.U32  Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 4*64)

VHADD.S8   D1,D2,D3 ; D1=D2+D3 (SIMD, 8*8)
VHADD.S16  D1,D2,D3 ; D1=D2+D3 (SIMD, 4*16)
VHADD.S32  D1,D2,D3 ; D1=D2+D3 (SIMD, 2*32)
VHADD.U8   D1,D2,D3 ; D1=D2+D3 (SIMD, 8*8)
VHADD.U16  D1,D2,D3 ; D1=D2+D3 (SIMD, 4*16)
VHADD.U32  D1,D2,D3 ; D1=D2+D3 (SIMD, 2*64)

; VHSUB

VHSUB.S8   Q1,Q2,Q3 ; Q1=Q2-Q3 (SIMD, 16*8)
VHSUB.S16  Q1,Q2,Q3 ; Q1=Q2-Q3 (SIMD, 8*16)
VHSUB.S32  Q1,Q2,Q3 ; Q1=Q2-Q3 (SIMD, 4*32)
VHSUB.U8   Q1,Q2,Q3 ; Q1=Q2-Q3 (SIMD, 16*8)
VHSUB.U16  Q1,Q2,Q3 ; Q1=Q2-Q3 (SIMD, 8*16)
VHSUB.U32  Q1,Q2,Q3 ; Q1=Q2-Q3 (SIMD, 4*64)

VHSUB.S8   D1,D2,D3 ; D1=D2-D3 (SIMD, 8*8)
VHSUB.S16  D1,D2,D3 ; D1=D2-D3 (SIMD, 4*16)
VHSUB.S32  D1,D2,D3 ; D1=D2-D3 (SIMD, 2*32)
VHSUB.U8   D1,D2,D3 ; D1=D2-D3 (SIMD, 8*8)
VHSUB.U16  D1,D2,D3 ; D1=D2-D3 (SIMD, 4*16)
VHSUB.U32  D1,D2,D3 ; D1=D2-D3 (SIMD, 2*64)

; VPADAL

VPADAL.S8   Q1,Q2 ; Q1+=pairwise_add(Q2) (SIMD, 8*8)
VPADAL.S16  Q1,Q2 ; Q1+=pairwise_add(Q2) (SIMD, 4*16)
VPADAL.S32  Q1,Q2 ; Q1+=pairwise_add(Q2) (SIMD, 2*32)
VPADAL.U8   Q1,Q2 ; Q1+=pairwise_add(Q2) (SIMD, 8*8)
VPADAL.U16  Q1,Q2 ; Q1+=pairwise_add(Q2) (SIMD, 4*16)
VPADAL.U32  Q1,Q2 ; Q1+=pairwise_add(Q2) (SIMD, 2*64)

VPADAL.S8   D1,D2 ; D1+=pairwise_add(D2) (SIMD, 4*8)
VPADAL.S16  D1,D2 ; D1+=pairwise_add(D2) (SIMD, 2*16)
VPADAL.S32  D1,D2 ; D1+=pairwise_add(D2) (SIMD, 1*32)
VPADAL.U8   D1,D2 ; D1+=pairwise_add(D2) (SIMD, 4*8)
VPADAL.U16  D1,D2 ; D1+=pairwise_add(D2) (SIMD, 2*16)
VPADAL.U32  D1,D2 ; D1+=pairwise_add(D2) (SIMD, 1*64)

; VPADD

VPADD.I8   D1,D2,D3 ; D1=pairwise_add(D2 and D3) (SIMD, 8*8)
VPADD.I16  D1,D2,D3 ; D1=pairwise_add(D2 and D3) (SIMD, 4*16)
VPADD.I32  D1,D2,D3 ; D1=pairwise_add(D2 and D3) (SIMD, 2*32)
VPADD.F32  D1,D2,D3 ; D1=pairwise_add(D2 and D3) (SIMD, 2*32)

; VPADDL

VPADDL.S8   Q1,Q2 ; Q1=pairwise_add(Q2) (SIMD, 8*8)
VPADDL.S16  Q1,Q2 ; Q1=pairwise_add(Q2) (SIMD, 4*16)
VPADDL.S32  Q1,Q2 ; Q1=pairwise_add(Q2) (SIMD, 2*32)
VPADDL.U8   Q1,Q2 ; Q1=pairwise_add(Q2) (SIMD, 8*8)
VPADDL.U16  Q1,Q2 ; Q1=pairwise_add(Q2) (SIMD, 4*16)
VPADDL.U32  Q1,Q2 ; Q1=pairwise_add(Q2) (SIMD, 2*64)

VPADDL.S8   D1,D2 ; D1=pairwise_add(D2) (SIMD, 4*8)
VPADDL.S16  D1,D2 ; D1=pairwise_add(D2) (SIMD, 2*16)
VPADDL.S32  D1,D2 ; D1=pairwise_add(D2) (SIMD, 1*32)
VPADDL.U8   D1,D2 ; D1=pairwise_add(D2) (SIMD, 4*8)
VPADDL.U16  D1,D2 ; D1=pairwise_add(D2) (SIMD, 2*16)
VPADDL.U32  D1,D2 ; D1=pairwise_add(D2) (SIMD, 1*64)

; VRADDHN

VRADDHN.I16 D1,Q2,Q3 ; D1=Q2+Q3 (SIMD, 8*16)
VRADDHN.I32 D1,Q2,Q3 ; D1=Q2+Q3 (SIMD, 4*32)
VRADDHN.I64 D1,Q2,Q3 ; D1=Q2+Q3 (SIMD, 2*64)

; VRHADD

VRHADD.S8   Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 16*8)
VRHADD.S16  Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 8*16)
VRHADD.S32  Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 4*32)
VRHADD.U8   Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 16*8)
VRHADD.U16  Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 8*16)
VRHADD.U32  Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 4*64)

VRHADD.S8   D1,D2,D3 ; D1=D2+D3 (SIMD, 8*8)
VRHADD.S16  D1,D2,D3 ; D1=D2+D3 (SIMD, 4*16)
VRHADD.S32  D1,D2,D3 ; D1=D2+D3 (SIMD, 2*32)
VRHADD.U8   D1,D2,D3 ; D1=D2+D3 (SIMD, 8*8)
VRHADD.U16  D1,D2,D3 ; D1=D2+D3 (SIMD, 4*16)
VRHADD.U32  D1,D2,D3 ; D1=D2+D3 (SIMD, 2*64)

; VRSUBHN

VRSUBHN.I16 D1,Q2,Q3 ; D1=Q2-Q3 (SIMD, 8*16)
VRSUBHN.I32 D1,Q2,Q3 ; D1=Q2-Q3 (SIMD, 4*32)
VRSUBHN.I64 D1,Q2,Q3 ; D1=Q2-Q3 (SIMD, 2*64)

; VQADD

VQADD.S8    Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 16*8)
VQADD.S16   Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 8*16)
VQADD.S32   Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 4*32)
VQADD.S64   Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 2*64)

VQADD.S8    D1,D2,D3 ; D1=D2+D3 (SIMD, 16*8)
VQADD.S16   D1,D2,D3 ; D1=D2+D3 (SIMD, 8*16)
VQADD.S32   D1,D2,D3 ; D1=D2+D3 (SIMD, 4*32)
VQADD.S64   D1,D2,D3 ; D1=D2+D3 (SIMD, 2*64)

VQADD.U8    Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 16*8)
VQADD.U16   Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 8*16)
VQADD.U32   Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 4*32)
VQADD.U64   Q1,Q2,Q3 ; Q1=Q2+Q3 (SIMD, 2*64)

VQADD.U8    D1,D2,D3 ; D1=D2+D3 (SIMD, 16*8)
VQADD.U16   D1,D2,D3 ; D1=D2+D3 (SIMD, 8*16)
VQADD.U32   D1,D2,D3 ; D1=D2+D3 (SIMD, 4*32)
VQADD.U64   D1,D2,D3 ; D1=D2+D3 (SIMD, 2*64)

; VQSUB

VQSUB.S8    Q1,Q2,Q3 ; Q1=Q2-Q3 (SIMD, 16*8)
VQSUB.S16   Q1,Q2,Q3 ; Q1=Q2-Q3 (SIMD, 8*16)
VQSUB.S32   Q1,Q2,Q3 ; Q1=Q2-Q3 (SIMD, 4*32)
VQSUB.S64   Q1,Q2,Q3 ; Q1=Q2-Q3 (SIMD, 2*64)

VQSUB.S8    D1,D2,D3 ; D1=D2-D3 (SIMD, 16*8)
VQSUB.S16   D1,D2,D3 ; D1=D2-D3 (SIMD, 8*16)
VQSUB.S32   D1,D2,D3 ; D1=D2-D3 (SIMD, 4*32)
VQSUB.S64   D1,D2,D3 ; D1=D2-D3 (SIMD, 2*64)

VQSUB.U8    Q1,Q2,Q3 ; Q1=Q2-Q3 (SIMD, 16*8)
VQSUB.U16   Q1,Q2,Q3 ; Q1=Q2-Q3 (SIMD, 8*16)
VQSUB.U32   Q1,Q2,Q3 ; Q1=Q2-Q3 (SIMD, 4*32)
VQSUB.U64   Q1,Q2,Q3 ; Q1=Q2-Q3 (SIMD, 2*64)

VQSUB.U8    D1,D2,D3 ; D1=D2-D3 (SIMD, 16*8)
VQSUB.U16   D1,D2,D3 ; D1=D2-D3 (SIMD, 8*16)
VQSUB.U32   D1,D2,D3 ; D1=D2-D3 (SIMD, 4*32)
VQSUB.U64   D1,D2,D3 ; D1=D2-D3 (SIMD, 2*64)

; VSUB

VSUB.I8    Q1,Q2,Q3 ; Q1=Q2-Q3 (SIMD, 16*8)
VSUB.I16   Q1,Q2,Q3 ; Q1=Q2-Q3 (SIMD, 8*16)
VSUB.I32   Q1,Q2,Q3 ; Q1=Q2-Q3 (SIMD, 4*32)
VSUB.I64   Q1,Q2,Q3 ; Q1=Q2-Q3 (SIMD, 2*64)

VSUB.I8    D1,D2,D3 ; D1=D2-D3 (SIMD, 16*8)
VSUB.I16   D1,D2,D3 ; D1=D2-D3 (SIMD, 8*16)
VSUB.I32   D1,D2,D3 ; D1=D2-D3 (SIMD, 4*32)
VSUB.I64   D1,D2,D3 ; D1=D2-D3 (SIMD, 2*64)

VSUB.F32   Q1,Q2,Q3 ; Q1=Q2-Q3 (SIMD, 4*32)
VSUB.F32   D1,D2,D3 ; D1=D2-D3 (SIMD, 2*32)

VSUB.F64   D1,D2,D3 ; D1=D2-D3 (VFP)
VSUBEQ.F64 D1,D2,D3 ; D1=D2-D3 (VFP)

VSUB.F32   S1,S2,S3 ; S1=S2-S3 (VFP)
VSUBEQ.F32 S1,S2,S3 ; S1=S2-S3 (VFP)

; VSUBHN

VSUBHN.I16 D1,Q2,Q3 ; D1=Q2+Q3 (SIMD, 8*16)
VSUBHN.I32 D1,Q2,Q3 ; D1=Q2+Q3 (SIMD, 4*32)
VSUBHN.I64 D1,Q2,Q3 ; D1=Q2+Q3 (SIMD, 2*64)

; VSUBL

VSUBL.S8   Q1,D2,D3 ; Q1=D2-D3 (SIMD, 8*8)
VSUBL.S16  Q1,D2,D3 ; Q1=D2-D3 (SIMD, 4*16)
VSUBL.S32  Q1,D2,D3 ; Q1=D2-D3 (SIMD, 2*32)
VSUBL.U8   Q1,D2,D3 ; Q1=D2-D3 (SIMD, 8*8)
VSUBL.U16  Q1,D2,D3 ; Q1=D2-D3 (SIMD, 4*16)
VSUBL.U32  Q1,D2,D3 ; Q1=D2-D3 (SIMD, 2*64)

; VSUBW

VSUBW.S8   Q1,Q2,D3 ; Q1=Q2-D3 (SIMD, 8*8)
VSUBW.S16  Q1,Q2,D3 ; Q1=Q2-D3 (SIMD, 4*16)
VSUBW.S32  Q1,Q2,D3 ; Q1=Q2-D3 (SIMD, 2*32)
VSUBW.U8   Q1,Q2,D3 ; Q1=Q2-D3 (SIMD, 8*8)
VSUBW.U16  Q1,Q2,D3 ; Q1=Q2-D3 (SIMD, 4*16)
VSUBW.U32  Q1,Q2,D3 ; Q1=Q2-D3 (SIMD, 2*64)

#end
