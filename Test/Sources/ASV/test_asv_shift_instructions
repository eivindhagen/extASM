
#name <Tmp$Dir>.^.^.test_asv_shift_instructions

#area test,code

; VSHL

VSHL.S8    Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 16*8)
VSHL.S16   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 8*16)
VSHL.S32   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 4*32)
VSHL.S64   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 2*64)
VSHL.U8    Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 16*8)
VSHL.U16   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 8*16)
VSHL.U32   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 4*32)
VSHL.U64   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 2*64)

VSHL.S8    D1,D2,D3 ; D1=D2 << D3 (SIMD, 8*8)
VSHL.S16   D1,D2,D3 ; D1=D2 << D3 (SIMD, 4*16)
VSHL.S32   D1,D2,D3 ; D1=D2 << D3 (SIMD, 2*32)
VSHL.S64   D1,D2,D3 ; D1=D2 << D3 (SIMD, 1*64)
VSHL.U8    D1,D2,D3 ; D1=D2 << D3 (SIMD, 8*8)
VSHL.U16   D1,D2,D3 ; D1=D2 << D3 (SIMD, 4*16)
VSHL.U32   D1,D2,D3 ; D1=D2 << D3 (SIMD, 2*32)
VSHL.U64   D1,D2,D3 ; D1=D2 << D3 (SIMD, 1*64)

VSHL.I8    Q1,Q2,#1 ; Q1=Q2 << 1 (SIMD, 16*8)
VSHL.I16   Q1,Q2,#1 ; Q1=Q2 << 1 (SIMD, 8*16)
VSHL.I32   Q1,Q2,#1 ; Q1=Q2 << 1 (SIMD, 4*32)
VSHL.I64   Q1,Q2,#1 ; Q1=Q2 << 1 (SIMD, 2*64)

VSHL.I8    D1,D2,#1 ; D1=D2 << 1 (SIMD, 8*8)
VSHL.I16   D1,D2,#1 ; D1=D2 << 1 (SIMD, 4*16)
VSHL.I32   D1,D2,#1 ; D1=D2 << 1 (SIMD, 2*32)
VSHL.I64   D1,D2,#1 ; D1=D2 << 1 (SIMD, 1*64)

; VSHLL

VSHLL.I8    Q1,D2,#8  ; Q1=D2 << 1 (SIMD, 8*8)
VSHLL.I16   Q1,D2,#16 ; Q1=D2 << 1 (SIMD, 4*16)
VSHLL.I32   Q1,D2,#32 ; Q1=D2 << 1 (SIMD, 2*32)
VSHLL.S8    Q1,D2,#1  ; Q1=D2 << 1 (SIMD, 8*8)
VSHLL.S16   Q1,D2,#1  ; Q1=D2 << 1 (SIMD, 4*16)
VSHLL.S32   Q1,D2,#1  ; Q1=D2 << 1 (SIMD, 2*32)
VSHLL.U8    Q1,D2,#1  ; Q1=D2 << 1 (SIMD, 8*8)
VSHLL.U16   Q1,D2,#1  ; Q1=D2 << 1 (SIMD, 4*16)
VSHLL.U32   Q1,D2,#1  ; Q1=D2 << 1 (SIMD, 2*32)

; VRSHL

VRSHL.S8    Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 16*8)
VRSHL.S16   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 8*16)
VRSHL.S32   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 4*32)
VRSHL.S64   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 2*64)
VRSHL.U8    Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 16*8)
VRSHL.U16   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 8*16)
VRSHL.U32   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 4*32)
VRSHL.U64   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 2*64)

VRSHL.S8    D1,D2,D3 ; D1=D2 << D3 (SIMD, 8*8)
VRSHL.S16   D1,D2,D3 ; D1=D2 << D3 (SIMD, 4*16)
VRSHL.S32   D1,D2,D3 ; D1=D2 << D3 (SIMD, 2*32)
VRSHL.S64   D1,D2,D3 ; D1=D2 << D3 (SIMD, 1*64)
VRSHL.U8    D1,D2,D3 ; D1=D2 << D3 (SIMD, 8*8)
VRSHL.U16   D1,D2,D3 ; D1=D2 << D3 (SIMD, 4*16)
VRSHL.U32   D1,D2,D3 ; D1=D2 << D3 (SIMD, 2*32)
VRSHL.U64   D1,D2,D3 ; D1=D2 << D3 (SIMD, 1*64)

; VQSHL

VQSHL.S8    Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 16*8)
VQSHL.S16   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 8*16)
VQSHL.S32   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 4*32)
VQSHL.S64   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 2*64)
VQSHL.U8    Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 16*8)
VQSHL.U16   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 8*16)
VQSHL.U32   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 4*32)
VQSHL.U64   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 2*64)

VQSHL.S8    D1,D2,D3 ; D1=D2 << D3 (SIMD, 8*8)
VQSHL.S16   D1,D2,D3 ; D1=D2 << D3 (SIMD, 4*16)
VQSHL.S32   D1,D2,D3 ; D1=D2 << D3 (SIMD, 2*32)
VQSHL.S64   D1,D2,D3 ; D1=D2 << D3 (SIMD, 1*64)
VQSHL.U8    D1,D2,D3 ; D1=D2 << D3 (SIMD, 8*8)
VQSHL.U16   D1,D2,D3 ; D1=D2 << D3 (SIMD, 4*16)
VQSHL.U32   D1,D2,D3 ; D1=D2 << D3 (SIMD, 2*32)
VQSHL.U64   D1,D2,D3 ; D1=D2 << D3 (SIMD, 1*64)

VQSHL.S8    Q1,Q2,#1 ; Q1=Q2 << 1 (SIMD, 16*8)
VQSHL.S16   Q1,Q2,#1 ; Q1=Q2 << 1 (SIMD, 8*16)
VQSHL.S32   Q1,Q2,#1 ; Q1=Q2 << 1 (SIMD, 4*32)
VQSHL.S64   Q1,Q2,#1 ; Q1=Q2 << 1 (SIMD, 2*64)
VQSHL.U8    Q1,Q2,#1 ; Q1=Q2 << 1 (SIMD, 16*8)
VQSHL.U16   Q1,Q2,#1 ; Q1=Q2 << 1 (SIMD, 8*16)
VQSHL.U32   Q1,Q2,#1 ; Q1=Q2 << 1 (SIMD, 4*32)
VQSHL.U64   Q1,Q2,#1 ; Q1=Q2 << 1 (SIMD, 2*64)

VQSHL.S8    D1,D2,#1 ; D1=D2 << 1 (SIMD, 16*8)
VQSHL.S16   D1,D2,#1 ; D1=D2 << 1 (SIMD, 8*16)
VQSHL.S32   D1,D2,#1 ; D1=D2 << 1 (SIMD, 4*32)
VQSHL.S64   D1,D2,#1 ; D1=D2 << 1 (SIMD, 2*64)
VQSHL.U8    D1,D2,#1 ; D1=D2 << 1 (SIMD, 16*8)
VQSHL.U16   D1,D2,#1 ; D1=D2 << 1 (SIMD, 8*16)
VQSHL.U32   D1,D2,#1 ; D1=D2 << 1 (SIMD, 4*32)
VQSHL.U64   D1,D2,#1 ; D1=D2 << 1 (SIMD, 2*64)

; VQSHLU

VQSHLU.S8    Q1,Q2,#1 ; Q1=Q2 << 1 (SIMD, 16*8)
VQSHLU.S16   Q1,Q2,#1 ; Q1=Q2 << 1 (SIMD, 8*16)
VQSHLU.S32   Q1,Q2,#1 ; Q1=Q2 << 1 (SIMD, 4*32)
VQSHLU.S64   Q1,Q2,#1 ; Q1=Q2 << 1 (SIMD, 2*64)

VQSHLU.S8    D1,D2,#1 ; D1=D2 << 1 (SIMD, 16*8)
VQSHLU.S16   D1,D2,#1 ; D1=D2 << 1 (SIMD, 8*16)
VQSHLU.S32   D1,D2,#1 ; D1=D2 << 1 (SIMD, 4*32)
VQSHLU.S64   D1,D2,#1 ; D1=D2 << 1 (SIMD, 2*64)

; VQRSHL

VQRSHL.S8    Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 16*8)
VQRSHL.S16   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 8*16)
VQRSHL.S32   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 4*32)
VQRSHL.S64   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 2*64)
VQRSHL.U8    Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 16*8)
VQRSHL.U16   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 8*16)
VQRSHL.U32   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 4*32)
VQRSHL.U64   Q1,Q2,Q3 ; Q1=Q2 << Q3 (SIMD, 2*64)

VQRSHL.S8    D1,D2,D3 ; D1=D2 << D3 (SIMD, 8*8)
VQRSHL.S16   D1,D2,D3 ; D1=D2 << D3 (SIMD, 4*16)
VQRSHL.S32   D1,D2,D3 ; D1=D2 << D3 (SIMD, 2*32)
VQRSHL.S64   D1,D2,D3 ; D1=D2 << D3 (SIMD, 1*64)
VQRSHL.U8    D1,D2,D3 ; D1=D2 << D3 (SIMD, 8*8)
VQRSHL.U16   D1,D2,D3 ; D1=D2 << D3 (SIMD, 4*16)
VQRSHL.U32   D1,D2,D3 ; D1=D2 << D3 (SIMD, 2*32)
VQRSHL.U64   D1,D2,D3 ; D1=D2 << D3 (SIMD, 1*64)

; VSLI

VSLI.8     Q1,Q2,#1 ; Q1|=Q2 << 1 (SIMD, 16*8)
VSLI.16    Q1,Q2,#1 ; Q1|=Q2 << 1 (SIMD, 8*16)
VSLI.32    Q1,Q2,#1 ; Q1|=Q2 << 1 (SIMD, 4*32)
VSLI.64    Q1,Q2,#1 ; Q1|=Q2 << 1 (SIMD, 2*64)

VSLI.8     D1,D2,#1 ; D1|=D2 << 1 (SIMD, 8*8)
VSLI.16    D1,D2,#1 ; D1|=D2 << 1 (SIMD, 4*16)
VSLI.32    D1,D2,#1 ; D1|=D2 << 1 (SIMD, 2*32)
VSLI.64    D1,D2,#1 ; D1|=D2 << 1 (SIMD, 1*64)

; VSHR

VSHR.S8    Q1,Q2,#1 ; Q1=Q2 >> 1 (SIMD, 16*8)
VSHR.S16   Q1,Q2,#1 ; Q1=Q2 >> 1 (SIMD, 8*16)
VSHR.S32   Q1,Q2,#1 ; Q1=Q2 >> 1 (SIMD, 4*32)
VSHR.S64   Q1,Q2,#1 ; Q1=Q2 >> 1 (SIMD, 2*64)
VSHR.U8    Q1,Q2,#1 ; Q1=Q2 >> 1 (SIMD, 16*8)
VSHR.U16   Q1,Q2,#1 ; Q1=Q2 >> 1 (SIMD, 8*16)
VSHR.U32   Q1,Q2,#1 ; Q1=Q2 >> 1 (SIMD, 4*32)
VSHR.U64   Q1,Q2,#1 ; Q1=Q2 >> 1 (SIMD, 2*64)

VSHR.S8    D1,D2,#1 ; D1=D2 >> 1 (SIMD, 8*8)
VSHR.S16   D1,D2,#1 ; D1=D2 >> 1 (SIMD, 4*16)
VSHR.S32   D1,D2,#1 ; D1=D2 >> 1 (SIMD, 2*32)
VSHR.S64   D1,D2,#1 ; D1=D2 >> 1 (SIMD, 1*64)
VSHR.U8    D1,D2,#1 ; D1=D2 >> 1 (SIMD, 8*8)
VSHR.U16   D1,D2,#1 ; D1=D2 >> 1 (SIMD, 4*16)
VSHR.U32   D1,D2,#1 ; D1=D2 >> 1 (SIMD, 2*32)
VSHR.U64   D1,D2,#1 ; D1=D2 >> 1 (SIMD, 1*64)

; VRSHR

VRSHR.S8    Q1,Q2,#1 ; Q1=Q2 >> 1 (SIMD, 16*8)
VRSHR.S16   Q1,Q2,#1 ; Q1=Q2 >> 1 (SIMD, 8*16)
VRSHR.S32   Q1,Q2,#1 ; Q1=Q2 >> 1 (SIMD, 4*32)
VRSHR.S64   Q1,Q2,#1 ; Q1=Q2 >> 1 (SIMD, 2*64)
VRSHR.U8    Q1,Q2,#1 ; Q1=Q2 >> 1 (SIMD, 16*8)
VRSHR.U16   Q1,Q2,#1 ; Q1=Q2 >> 1 (SIMD, 8*16)
VRSHR.U32   Q1,Q2,#1 ; Q1=Q2 >> 1 (SIMD, 4*32)
VRSHR.U64   Q1,Q2,#1 ; Q1=Q2 >> 1 (SIMD, 2*64)

VRSHR.S8    D1,D2,#1 ; D1=D2 >> 1 (SIMD, 8*8)
VRSHR.S16   D1,D2,#1 ; D1=D2 >> 1 (SIMD, 4*16)
VRSHR.S32   D1,D2,#1 ; D1=D2 >> 1 (SIMD, 2*32)
VRSHR.S64   D1,D2,#1 ; D1=D2 >> 1 (SIMD, 1*64)
VRSHR.U8    D1,D2,#1 ; D1=D2 >> 1 (SIMD, 8*8)
VRSHR.U16   D1,D2,#1 ; D1=D2 >> 1 (SIMD, 4*16)
VRSHR.U32   D1,D2,#1 ; D1=D2 >> 1 (SIMD, 2*32)
VRSHR.U64   D1,D2,#1 ; D1=D2 >> 1 (SIMD, 1*64)

; VQSHRN

VQSHRN.S16   D1,Q2,#1 ; D1=Q2 >> 1 (SIMD, 8*16)
VQSHRN.S32   D1,Q2,#1 ; D1=Q2 >> 1 (SIMD, 4*32)
VQSHRN.S64   D1,Q2,#1 ; D1=Q2 >> 1 (SIMD, 2*64)
VQSHRN.U16   D1,Q2,#1 ; D1=Q2 >> 1 (SIMD, 8*16)
VQSHRN.U32   D1,Q2,#1 ; D1=Q2 >> 1 (SIMD, 4*32)
VQSHRN.U64   D1,Q2,#1 ; D1=Q2 >> 1 (SIMD, 2*64)

; VQSHRUN

VQSHRUN.S16   D1,Q2,#1 ; D1=Q2 >> 1 (SIMD, 8*16)
VQSHRUN.S32   D1,Q2,#1 ; D1=Q2 >> 1 (SIMD, 4*32)
VQSHRUN.S64   D1,Q2,#1 ; D1=Q2 >> 1 (SIMD, 2*64)

; VQRSHRN

VQRSHRN.S16   D1,Q2,#1 ; D1=Q2 >> 1 (SIMD, 8*16)
VQRSHRN.S32   D1,Q2,#1 ; D1=Q2 >> 1 (SIMD, 4*32)
VQRSHRN.S64   D1,Q2,#1 ; D1=Q2 >> 1 (SIMD, 2*64)
VQRSHRN.U16   D1,Q2,#1 ; D1=Q2 >> 1 (SIMD, 8*16)
VQRSHRN.U32   D1,Q2,#1 ; D1=Q2 >> 1 (SIMD, 4*32)
VQRSHRN.U64   D1,Q2,#1 ; D1=Q2 >> 1 (SIMD, 2*64)

; VQRSHRUN

VQRSHRUN.S16   D1,Q2,#1 ; D1=Q2 >> 1 (SIMD, 8*16)
VQRSHRUN.S32   D1,Q2,#1 ; D1=Q2 >> 1 (SIMD, 4*32)
VQRSHRUN.S64   D1,Q2,#1 ; D1=Q2 >> 1 (SIMD, 2*64)

; VSRI

VSRI.8     Q1,Q2,#1 ; Q1|=Q2 >> 1 (SIMD, 16*8)
VSRI.16    Q1,Q2,#1 ; Q1|=Q2 >> 1 (SIMD, 8*16)
VSRI.32    Q1,Q2,#1 ; Q1|=Q2 >> 1 (SIMD, 4*32)
VSRI.64    Q1,Q2,#1 ; Q1|=Q2 >> 1 (SIMD, 2*64)

VSRI.8     D1,D2,#1 ; D1|=D2 >> 1 (SIMD, 8*8)
VSRI.16    D1,D2,#1 ; D1|=D2 >> 1 (SIMD, 4*16)
VSRI.32    D1,D2,#1 ; D1|=D2 >> 1 (SIMD, 2*32)
VSRI.64    D1,D2,#1 ; D1|=D2 >> 1 (SIMD, 1*64)

; VSRA

VSRA.S8    Q1,Q2,#1 ; Q1+=Q2 >> 1 (SIMD, 16*8)
VSRA.S16   Q1,Q2,#1 ; Q1+=Q2 >> 1 (SIMD, 8*16)
VSRA.S32   Q1,Q2,#1 ; Q1+=Q2 >> 1 (SIMD, 4*32)
VSRA.S64   Q1,Q2,#1 ; Q1+=Q2 >> 1 (SIMD, 2*64)
VSRA.U8    Q1,Q2,#1 ; Q1+=Q2 >> 1 (SIMD, 16*8)
VSRA.U16   Q1,Q2,#1 ; Q1+=Q2 >> 1 (SIMD, 8*16)
VSRA.U32   Q1,Q2,#1 ; Q1+=Q2 >> 1 (SIMD, 4*32)
VSRA.U64   Q1,Q2,#1 ; Q1+=Q2 >> 1 (SIMD, 2*64)

VSRA.S8    D1,D2,#1 ; D1+=D2 >> 1 (SIMD, 8*8)
VSRA.S16   D1,D2,#1 ; D1+=D2 >> 1 (SIMD, 4*16)
VSRA.S32   D1,D2,#1 ; D1+=D2 >> 1 (SIMD, 2*32)
VSRA.S64   D1,D2,#1 ; D1+=D2 >> 1 (SIMD, 1*64)
VSRA.U8    D1,D2,#1 ; D1+=D2 >> 1 (SIMD, 8*8)
VSRA.U16   D1,D2,#1 ; D1+=D2 >> 1 (SIMD, 4*16)
VSRA.U32   D1,D2,#1 ; D1+=D2 >> 1 (SIMD, 2*32)
VSRA.U64   D1,D2,#1 ; D1+=D2 >> 1 (SIMD, 1*64)

; VRSRA

VRSRA.S8    Q1,Q2,#1 ; Q1+=Q2 >> 1 (SIMD, 16*8)
VRSRA.S16   Q1,Q2,#1 ; Q1+=Q2 >> 1 (SIMD, 8*16)
VRSRA.S32   Q1,Q2,#1 ; Q1+=Q2 >> 1 (SIMD, 4*32)
VRSRA.S64   Q1,Q2,#1 ; Q1+=Q2 >> 1 (SIMD, 2*64)
VRSRA.U8    Q1,Q2,#1 ; Q1+=Q2 >> 1 (SIMD, 16*8)
VRSRA.U16   Q1,Q2,#1 ; Q1+=Q2 >> 1 (SIMD, 8*16)
VRSRA.U32   Q1,Q2,#1 ; Q1+=Q2 >> 1 (SIMD, 4*32)
VRSRA.U64   Q1,Q2,#1 ; Q1+=Q2 >> 1 (SIMD, 2*64)

VRSRA.S8    D1,D2,#1 ; D1+=D2 >> 1 (SIMD, 8*8)
VRSRA.S16   D1,D2,#1 ; D1+=D2 >> 1 (SIMD, 4*16)
VRSRA.S32   D1,D2,#1 ; D1+=D2 >> 1 (SIMD, 2*32)
VRSRA.S64   D1,D2,#1 ; D1+=D2 >> 1 (SIMD, 1*64)
VRSRA.U8    D1,D2,#1 ; D1+=D2 >> 1 (SIMD, 8*8)
VRSRA.U16   D1,D2,#1 ; D1+=D2 >> 1 (SIMD, 4*16)
VRSRA.U32   D1,D2,#1 ; D1+=D2 >> 1 (SIMD, 2*32)
VRSRA.U64   D1,D2,#1 ; D1+=D2 >> 1 (SIMD, 1*64)

#end
